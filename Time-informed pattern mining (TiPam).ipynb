{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GSPG(single_sequence, sequences_list):\n",
    "\n",
    "### find the list of sequences which consist of the subset_seq\n",
    "    def find_subset_sequences(subset_seq, set_of_seqs):\n",
    "        result = []\n",
    "        for seq in set_of_seqs:\n",
    "            subset_index = 0\n",
    "            for item in seq:\n",
    "                if item[0] == subset_seq[subset_index]:\n",
    "                    subset_index += 1\n",
    "                    if subset_index == len(subset_seq):\n",
    "                        result.append(seq)\n",
    "                        break\n",
    "        return result\n",
    "    ### generates all possible sequences where single_sequence can be embedded within a larger sequence, \n",
    "    ### filling non-matching positions with zeros.\n",
    "    def generate_sequences(subset, larger):\n",
    "        def recurse(subset_idx, last_idx, current_sequence):\n",
    "            if subset_idx == len(subset):\n",
    "                sequences.append(current_sequence.copy())\n",
    "                return\n",
    "            for i in range(last_idx + 1, len(larger)):\n",
    "                if larger[i][0] == subset[subset_idx]:\n",
    "                    new_sequence = current_sequence.copy()\n",
    "                    new_sequence[i] = larger[i]\n",
    "                    recurse(subset_idx + 1, i, new_sequence)\n",
    "\n",
    "        sequences = []\n",
    "        recurse(0, -1, [(0, '0:00-0:00')] * len(larger))\n",
    "        return sequences\n",
    "    \n",
    "    matching_sequences = find_subset_sequences(single_sequence, sequences_list)\n",
    "    \n",
    "    pre_seq = []\n",
    "    for seq in matching_sequences:\n",
    "        zero_seq = generate_sequences(single_sequence, seq)\n",
    "        pre_seq.append(zero_seq)\n",
    "\n",
    "    return pre_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the output will show the time Gap (median value) between each pair of elements in single_sequence\n",
    "\n",
    "import datetime\n",
    "import statistics\n",
    "\n",
    "def parse_time_interval(interval):\n",
    "    start, end = interval.split('-')\n",
    "    start = datetime.datetime.strptime(start, '%H:%M')\n",
    "    end = datetime.datetime.strptime(end, '%H:%M')\n",
    "    return start, end\n",
    "\n",
    "def calculate_time_distance(start_time, end_time):\n",
    "    delta = end_time - start_time\n",
    "    return delta.total_seconds() / 60  # Convert to minutes\n",
    "\n",
    "def get_time_distances(dataset, se1):\n",
    "    distances = { (se1[i], se1[i+1]): [] for i in range(len(se1)-1) }\n",
    "\n",
    "    for day_index, day_data in enumerate(dataset):\n",
    "        seen_pairs = set()\n",
    "        for sequence in day_data:\n",
    "            for i in range(len(sequence) - 1):\n",
    "                value1, interval1 = sequence[i]\n",
    "                value2, interval2 = sequence[i+1]\n",
    "                if (value1, value2) in distances:\n",
    "                    start_time1, end_time1 = parse_time_interval(interval1)\n",
    "                    start_time2, end_time2 = parse_time_interval(interval2)\n",
    "                    distance = calculate_time_distance(end_time1, start_time2)\n",
    "                    pair = (value1, value2, interval1, interval2, day_index)\n",
    "                    if pair not in seen_pairs:\n",
    "                        distances[(value1, value2)].append(distance)\n",
    "                        seen_pairs.add(pair)\n",
    "                        #print(seen_pairs)\n",
    "\n",
    "    return distances\n",
    "\n",
    "def calculate_median_distances(distances):\n",
    "    median_distances = {}\n",
    "    for pair, times in distances.items():\n",
    "        if times:\n",
    "            median_distances[pair] = statistics.median(times)\n",
    "        else:\n",
    "            median_distances[pair] = None\n",
    "    return median_distances\n",
    "\n",
    "def analyze_time_distances(dataset, se1):\n",
    "    time_distances = get_time_distances(dataset, se1)\n",
    "    median_distances = calculate_median_distances(time_distances)\n",
    "    result = [(pair[0], pair[1], median) for pair, median in median_distances.items()]\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### median duration and median starting time of extracted frequent patterns\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "def duration_starting(dataset, sequence):\n",
    "    # Function to convert time string to minutes from midnight\n",
    "    def time_to_minutes(t):\n",
    "        h, m = map(int, t.split(':'))\n",
    "        return h * 60 + m\n",
    "\n",
    "    # Function to calculate duration in minutes\n",
    "    def duration_to_minutes(start, end):\n",
    "        start_minutes = time_to_minutes(start)\n",
    "        end_minutes = time_to_minutes(end)\n",
    "        if end_minutes < start_minutes:\n",
    "            end_minutes += 24 * 60  # Adjust for times crossing midnight\n",
    "        return end_minutes - start_minutes\n",
    "\n",
    "    # Function to find median\n",
    "    def median(lst):\n",
    "        return np.median(np.array(lst))\n",
    "\n",
    "    # Process data\n",
    "    result = []\n",
    "    for idx, val in enumerate(sequence):\n",
    "        start_times = []\n",
    "        durations = []\n",
    "        unique_intervals = set()\n",
    "        for group_index, group in enumerate(dataset):\n",
    "            for day in group:\n",
    "                interval = day[idx]\n",
    "                # Use a tuple (group_index, interval) to ensure uniqueness across groups\n",
    "                unique_key = (group_index, interval)\n",
    "                if unique_key not in unique_intervals:\n",
    "                    unique_intervals.add(unique_key)\n",
    "                    start, end = interval[1].split('-')\n",
    "                    start_times.append(time_to_minutes(start))\n",
    "                    durations.append(duration_to_minutes(start, end))\n",
    "        \n",
    "        median_start_time = median(start_times)\n",
    "        median_duration = median(durations)\n",
    "        \n",
    "        # Convert median start time back to HH:MM format\n",
    "        median_start_time_str = str(timedelta(minutes=int(median_start_time)))[:-3]\n",
    "        \n",
    "        result.append((val, median_start_time_str, median_duration))\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from itertools import combinations\n",
    "\n",
    "def HGSP(len_GSP, min_support, data):\n",
    "    try:\n",
    "        dataset = [[item[0] for item in sublist] for sublist in data]\n",
    "        \n",
    "        def frozenset_to_list(itemset):\n",
    "            return list(itemset)\n",
    "        \n",
    "        def clean_and_convert(itemset):\n",
    "            inner_tuple = itemset[0]\n",
    "            return list(inner_tuple)\n",
    "        \n",
    "        def remove_zero_elements(data):\n",
    "            return [[[(a, b) for (a, b) in sublist if (a, b) != (0, '0:00-0:00')] for sublist in main_list] for main_list in data]\n",
    "\n",
    "        if len_GSP == 1:\n",
    "            unique_items = set(item for sublist in dataset for item in sublist)\n",
    "            binary_data = []\n",
    "            for item_combinations in dataset:\n",
    "                binary_sequence = {item: 1 if item in item_combinations else 0 for item in unique_items}\n",
    "                binary_data.append(binary_sequence)\n",
    "                \n",
    "            df = pd.DataFrame(binary_data)\n",
    "            \n",
    "            # Apply Apriori algorithm to find frequent sequential patterns of length 1\n",
    "            frequent_patterns = apriori(df, min_support=min_support, use_colnames=True)\n",
    "            FP = frequent_patterns[frequent_patterns['itemsets'].apply(lambda x: len(x) == 1)]\n",
    "            FP['itemsets'] = FP['itemsets'].apply(frozenset_to_list)\n",
    "            #FP['GSP'] = FP['itemsets'].apply(clean_and_convert)\n",
    "            Time_Gap = []\n",
    "            for sequ in FP['itemsets']:\n",
    "                result_ses = GSPG(sequ, data)\n",
    "                result_ses = remove_zero_elements(result_ses)\n",
    "                gaps = analyze_time_distances(result_ses, sequ)\n",
    "                Time_Gap.append(gaps)\n",
    "            FP['Time Gaps'] = Time_Gap\n",
    "            \n",
    "            StartingT_Duration = []\n",
    "            for sequ in FP['itemsets']:\n",
    "                result_ses = GSPG(sequ, data)\n",
    "                result_ses = remove_zero_elements(result_ses)\n",
    "                starting_duration = duration_starting(result_ses, sequ)\n",
    "                StartingT_Duration.append(starting_duration)\n",
    "            FP['Starting Time and Duration'] = StartingT_Duration\n",
    "\n",
    "            if FP.empty:\n",
    "                print(\"no output\")\n",
    "            else:\n",
    "                return FP\n",
    "        \n",
    "        else:\n",
    "            combinations_sequences = []\n",
    "            for sequence in dataset:\n",
    "                combinations_sequence = list(combinations(sequence, len_GSP))\n",
    "                combinations_sequences.append(combinations_sequence)\n",
    "            \n",
    "            unique_items = set(item for sublist in combinations_sequences for item in sublist)\n",
    "            \n",
    "            binary_data = []\n",
    "            for item_combinations in combinations_sequences:\n",
    "                binary_sequence = {item: 1 if item in item_combinations else 0 for item in unique_items}\n",
    "                binary_data.append(binary_sequence)\n",
    "            \n",
    "            df = pd.DataFrame(binary_data)\n",
    "            \n",
    "            # Apply Apriori algorithm to find frequent sequential patterns of length len_GSP\n",
    "            frequent_patterns = apriori(df, min_support=min_support, use_colnames=True)\n",
    "            FP = frequent_patterns[frequent_patterns['itemsets'].apply(lambda x: len(x) == 1)]\n",
    "            \n",
    "            # Convert frozensets to lists and clean the itemsets\n",
    "            FP['itemsets'] = FP['itemsets'].apply(frozenset_to_list)\n",
    "            FP['GSP'] = FP['itemsets'].apply(clean_and_convert)\n",
    "            Time_Gap = []\n",
    "            for sequ in FP['GSP']:\n",
    "                result_ses = GSPG(sequ, data)\n",
    "                result_ses = remove_zero_elements(result_ses)\n",
    "                gaps = analyze_time_distances(result_ses, sequ)\n",
    "                Time_Gap.append(gaps)\n",
    "            FP['Time Gaps'] = Time_Gap\n",
    "            \n",
    "            StartingT_Duration = []\n",
    "            for sequ in FP['GSP']:\n",
    "                result_ses = GSPG(sequ, data)\n",
    "                result_ses = remove_zero_elements(result_ses)\n",
    "                starting_duration = duration_starting(result_ses, sequ)\n",
    "                StartingT_Duration.append(starting_duration)\n",
    "            FP['Starting Time and Duration'] = StartingT_Duration\n",
    "\n",
    "            if FP.empty:\n",
    "                print(\"no output\")\n",
    "            else:\n",
    "                return FP\n",
    "    except Exception as e:\n",
    "        print(\"no output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### an example\n",
    "df1 = [\n",
    "    [ (3, '10:46-14:21'), (1, '14:22-14:54'),(7, '15:01-15:58'),(8, '18:53-20:04')],\n",
    "    [(9, '10:04-16:44'), (3, '11:33-19:34')],\n",
    "    [(10, '8:04-8:05'), (6, '9:01-11:33'), (3, '11:54-11:55')],\n",
    "    [(3, '10:01-12:02'),(7, '14:41-15:52'),(8, '19:53-19:54')],\n",
    "    [(3, '6:51-7:45'),(7, '9:44-10:01'), (1, '12:34-13:45'), (4, '14:22-17:54'),(8, '16:53-19:34')],\n",
    "    [(3, '7:46-8:21'),(6, '9:01-12:02'),(7, '14:41-15:52')],\n",
    "    [(3, '7:11-7:35'),(10, '7:44-8:12'), (7, '9:34-13:45'), (2, '17:22-19:54'),(8, '19:59-21:04')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP = HGSP(1,0.5,df1)\n",
    "print(GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP = HGSP(2,0.5,df1)\n",
    "print(GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GP = HGSP(3,0.5,df1)\n",
    "print(GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
